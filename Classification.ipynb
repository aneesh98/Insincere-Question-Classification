{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aneesh98/Insincere-Question-Classification/blob/master/Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsCV2IR39sf2",
        "colab_type": "code",
        "outputId": "646446e1-1f46-4595-e73c-e737de0a4391",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "############## DATA SETUP ##############\n",
        "!pip install kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.6)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.6.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2019.9.11)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.21.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzPIjkok_bSe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "ba4eded2-3794-4602-f836-1f3d466d45e3"
      },
      "source": [
        "!kaggle competitions download -c quora-insincere-questions-classification"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading train.csv.zip to /content\n",
            " 90% 49.0M/54.4M [00:00<00:00, 36.3MB/s]\n",
            "100% 54.4M/54.4M [00:00<00:00, 69.7MB/s]\n",
            "Downloading embeddings.zip to /content\n",
            "100% 5.94G/5.96G [01:10<00:00, 76.2MB/s]\n",
            "100% 5.96G/5.96G [01:10<00:00, 90.4MB/s]\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "100% 4.08M/4.08M [00:00<00:00, 40.3MB/s]\n",
            "\n",
            "Downloading test.csv.zip to /content\n",
            " 32% 5.00M/15.7M [00:00<00:00, 36.2MB/s]\n",
            "100% 15.7M/15.7M [00:00<00:00, 61.4MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8RMMNoQAqOe",
        "colab_type": "code",
        "outputId": "5595f7eb-0b95-4eb0-9e5b-14edb971daa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "!unzip train.csv.zip\n",
        "!unzip test.csv.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  train.csv.zip\n",
            "  inflating: train.csv               \n",
            "Archive:  test.csv.zip\n",
            "  inflating: test.csv                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5piGkqABF2j",
        "colab_type": "code",
        "outputId": "aac1eb78-41f7-49ee-f5a2-088dbb4b34cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "############# ALL IMPORT STATEMENTS ##############\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import nltk \n",
        "nltk.download('punkt')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NA69mq4sBPrw",
        "colab_type": "code",
        "outputId": "787511e5-d709-4801-91bf-429545d5c2d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "negative_texts = train[train['target']==1].head()\n",
        "for k in range(5):\n",
        "  print(negative_texts.iloc[k, 1])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Has the United States become the largest dictatorship in the world?\n",
            "Which babies are more sweeter to their parents? Dark skin babies or light skin babies?\n",
            "If blacks support school choice and mandatory sentencing for criminals why don't they vote Republican?\n",
            "I am gay boy and I love my cousin (boy). He is sexy, but I dont know what to do. He is hot, and I want to see his di**. What should I do?\n",
            "Which races have the smallest penis?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAhjZlLjpsWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CONTRACTION_MAP = {\n",
        "\"ain't\": \"is not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he'll've\": \"he he will have\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"I'd\": \"I would\",\n",
        "\"I'd've\": \"I would have\",\n",
        "\"I'll\": \"I will\",\n",
        "\"I'll've\": \"I will have\",\n",
        "\"I'm\": \"I am\",\n",
        "\"I've\": \"I have\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'd've\": \"i would have\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'll've\": \"i will have\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it'll've\": \"it will have\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she'll've\": \"she will have\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so as\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they'll've\": \"they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what'll've\": \"what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who'll've\": \"who will have\",\n",
        "\"who's\": \"who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you'll've\": \"you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\"\n",
        "}\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CIcdafTpxdU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "1464ce62-0ed3-4bec-c400-1076d9c1524d"
      },
      "source": [
        "class TextProcessor:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def expand_contractions(self, text, contraction_map = CONTRACTION_MAP):\n",
        "    pattern = re.compile('({})'.format('|'.join(contraction_map.keys())), flags = re.IGNORECASE | re.DOTALL)\n",
        "    def expand_match(contraction):\n",
        "      match = contraction.group(0)\n",
        "      print(\"GROUP \", match)\n",
        "      first_char = match[0]\n",
        "      expanded_contraction = contraction_map.get(match) if contraction_map.get(match) else contraction_map.get(match.lower())\n",
        "      expanded_contraction = first_char + expanded_contraction[1:]\n",
        "      return expanded_contraction\n",
        "    expanded_text = pattern.sub(expand_match, text)\n",
        "    # print(expanded_text)\n",
        "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
        "    return expanded_text\n",
        "\n",
        "  def remove_special_characters(self, text, remove_digits = False):\n",
        "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
        "    text = re.sub(pattern,'', text)\n",
        "    return text\n",
        "  \n",
        "  def lower_all(self, text):\n",
        "    return text.lower()\n",
        "\n",
        "  def remove_stopwords(self, text):\n",
        "    GAP_PATTERN = r'\\s+'\n",
        "    # nltk.download('stopwords')\n",
        "    stop_words = nltk.corpus.stopwords.words('english')\n",
        "    regex_wt = nltk.RegexpTokenizer(pattern=GAP_PATTERN, gaps = True).tokenize\n",
        "    tokens = regex_wt(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    return filtered_text\n",
        "\n",
        "  def pipeline(self, text):\n",
        "    text = self.expand_contractions(text)\n",
        "    text = self.remove_special_characters(text)\n",
        "    text = self.lower_all(text)\n",
        "    text = self.remove_stopwords(text)\n",
        "    return text\n",
        "\n",
        "string = \"Y'all can't expand contractions the I'd think %%@@###???\"\n",
        "tp = TextProcessor()\n",
        "tp.pipeline(string)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GROUP  Y'all\n",
            "GROUP  can't\n",
            "GROUP  I'd\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cannot expand contractions would think'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGVyivRglcUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "default_wt = nltk.word_tokenize\n",
        "treebank_wt = nltk.TreebankWordTokenizer()\n",
        "default_words = []\n",
        "treebank_words =[]\n",
        "\n",
        "for j in range(5):\n",
        "  default_words.append(regex_wt(negative_texts.iloc[j,1]))\n",
        "  treebank_words.append(treebank_wt.tokenize(negative_texts.iloc[j,1]))\n",
        "print(default_words)\n",
        "print(treebank_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm7fckQh240w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tproc = TextProcessor()\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense\n",
        "data = train\n",
        "max_features = 50000\n",
        "maxlen = 20\n",
        "X = data['question_text']\n",
        "y = data['target']\n",
        "for j in range(X.shape[0]):\n",
        "  if j%100==0:\n",
        "    print(j,\"/\",X.shape[0],\" Done\")\n",
        "  X[j] = tproc.pipeline(X[j])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMFfMkb1_Aqi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Embedding\n",
        "from keras import preprocessing\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDC72UI2gHSP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "class MyModel(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self):\n",
        "    self.max_words = 50000\n",
        "    self.max_len = 20\n",
        "    self.emb_dim = 10\n",
        "    self.epochs = 10\n",
        "    self.batch_size = 32\n",
        "    self.validation_split = 0.2\n",
        "    self.tokenizer = Tokenizer(num_words = self.max_len)\n",
        "    self.model = self.model()\n",
        "\n",
        "  def model(self): \n",
        "    model = Sequential()\n",
        "    model.add(Embedding(self.max_words, self.emb_dim, input_length = self.max_len))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "    model.compile(optimizer = 'adam', loss='categorical_crossentropy', metrics = ['acc'])\n",
        "    return model\n",
        "  \n",
        "  def pad_sequence(self, texts):\n",
        "    sequence = preprocessing.sequence.pad_sequences(texts, maxlen = self.max_len)\n",
        "    return sequence\n",
        "  \n",
        "  def tokenize_text(self, texts):\n",
        "    return self.tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "  def transform(self, texts):\n",
        "    texts = self.tokenize_text(texts)\n",
        "    texts = self.pad_sequence(texts)\n",
        "    return texts\n",
        "\n",
        "  def predict(self, text):\n",
        "    text = self.transform(text)\n",
        "    return self.model.predict(text)\n",
        "  \n",
        "  def predict_proba(self, text):\n",
        "    text = self.transform(text)\n",
        "    return self.model.predict(text)\n",
        "  \n",
        "  def fit(self, X, y):\n",
        "    ohe = OneHotEncoder()\n",
        "    y = np.array(y).reshape(-1,1)\n",
        "    y = ohe.fit_transform(y).toarray()\n",
        "    x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.25)\n",
        "    self.tokenizer.fit_on_texts(x_train)\n",
        "    x_train = self.transform(x_train)\n",
        "    history = self.model.fit(x_train, y_train, epochs = self.epochs, batch_size = self.batch_size, validation_split=self.validation_split)\n",
        "    return history\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drJTkG-dw0Gr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "77ea98ba-f734-42be-d734-bc7d90c978e5"
      },
      "source": [
        "x = np.array([1,1,1,2,2])\n",
        "ohe = OneHotEncoder()\n",
        "x = ohe.fit_transform(x.reshape(-1,1)).toarray()\n",
        "print(x)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0EZeDxzrkXi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "outputId": "5cf86da3-4ebb-4d00-fb67-fc6986e02269"
      },
      "source": [
        "new_model = MyModel()\n",
        "history = new_model.fit(X, y)"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 783672 samples, validate on 195919 samples\n",
            "Epoch 1/10\n",
            "783672/783672 [==============================] - 81s 104us/step - loss: 0.1963 - acc: 0.9379 - val_loss: 0.1944 - val_acc: 0.9384\n",
            "Epoch 2/10\n",
            "783672/783672 [==============================] - 81s 103us/step - loss: 0.1946 - acc: 0.9381 - val_loss: 0.1942 - val_acc: 0.9384\n",
            "Epoch 3/10\n",
            "783672/783672 [==============================] - 80s 103us/step - loss: 0.1946 - acc: 0.9381 - val_loss: 0.1944 - val_acc: 0.9385\n",
            "Epoch 4/10\n",
            "783672/783672 [==============================] - 80s 102us/step - loss: 0.1945 - acc: 0.9382 - val_loss: 0.1939 - val_acc: 0.9385\n",
            "Epoch 5/10\n",
            "783672/783672 [==============================] - 79s 101us/step - loss: 0.1944 - acc: 0.9382 - val_loss: 0.1964 - val_acc: 0.9381\n",
            "Epoch 6/10\n",
            "783672/783672 [==============================] - 80s 102us/step - loss: 0.1944 - acc: 0.9382 - val_loss: 0.1940 - val_acc: 0.9384\n",
            "Epoch 7/10\n",
            "783672/783672 [==============================] - 80s 102us/step - loss: 0.1943 - acc: 0.9381 - val_loss: 0.1942 - val_acc: 0.9383\n",
            "Epoch 8/10\n",
            "783672/783672 [==============================] - 81s 103us/step - loss: 0.1943 - acc: 0.9381 - val_loss: 0.1941 - val_acc: 0.9384\n",
            "Epoch 9/10\n",
            "783672/783672 [==============================] - 81s 103us/step - loss: 0.1943 - acc: 0.9382 - val_loss: 0.1940 - val_acc: 0.9385\n",
            "Epoch 10/10\n",
            "783672/783672 [==============================] - 82s 104us/step - loss: 0.1943 - acc: 0.9381 - val_loss: 0.1940 - val_acc: 0.9385\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCLrd521FFYF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "5f5648d7-8fea-4faf-8d94-bfad892747ec"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEICAYAAAC0+DhzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+VhyMMYYk48lB2OMMfn8PxpYf+j9j8P+AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8llXxtn2H1L4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bb802392-aedc-48ec-c138-ec4960d767e3"
      },
      "source": [
        "test_text = [\"Should People be alive or should they die horrible death?\"]\n",
        "val = new_model.predict(test_text[0])\n",
        "print(val)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.94169587 0.05830408]\n",
            " [0.9380862  0.06191375]\n",
            " [0.96204644 0.03795353]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.96204644 0.03795353]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.96204644 0.03795353]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.94169587 0.05830408]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9RHDyX5Z5c7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "7ab5a927-b325-4f7b-cb11-288099fef584"
      },
      "source": [
        "!pip install eli5"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting eli5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/2f/c85c7d8f8548e460829971785347e14e45fa5c6617da374711dec8cb38cc/eli5-0.10.1-py2.py3-none-any.whl (105kB)\n",
            "\r\u001b[K     |                             | 10kB 21.5MB/s eta 0:00:01\r\u001b[K     |                         | 20kB 3.1MB/s eta 0:00:01\r\u001b[K     |                      | 30kB 4.5MB/s eta 0:00:01\r\u001b[K     |                   | 40kB 3.1MB/s eta 0:00:01\r\u001b[K     |                | 51kB 3.8MB/s eta 0:00:01\r\u001b[K     |             | 61kB 4.4MB/s eta 0:00:01\r\u001b[K     |          | 71kB 5.1MB/s eta 0:00:01\r\u001b[K     |       | 81kB 5.8MB/s eta 0:00:01\r\u001b[K     |    | 92kB 6.4MB/s eta 0:00:01\r\u001b[K     | | 102kB 5.1MB/s eta 0:00:01\r\u001b[K     || 112kB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>16.0.0 in /usr/local/lib/python3.6/dist-packages (from eli5) (19.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from eli5) (1.12.0)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from eli5) (0.8.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from eli5) (1.3.2)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from eli5) (0.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from eli5) (2.10.3)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from eli5) (1.17.4)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.6/dist-packages (from eli5) (0.21.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->eli5) (1.1.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18->eli5) (0.14.0)\n",
            "Installing collected packages: eli5\n",
            "Successfully installed eli5-0.10.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yy-wVnvrZY8s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "12f5c73e-5256-48a5-c4ec-e83c704a6826"
      },
      "source": [
        "import eli5\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "categories = ['alt.atheism', 'soc.religion.christian',\n",
        "              'comp.graphics', 'sci.med']\n",
        "x1 = train.loc[train['target'] == 0, 'question_text']\n",
        "x1 = x1[0]\n",
        "print(new_model.predict(test_t))\n",
        "twenty_test = fetch_20newsgroups(\n",
        "    subset='test',\n",
        "    categories=categories,\n",
        "    shuffle=True,\n",
        "    random_state=42,\n",
        "    remove=('headers', 'footers'),\n",
        ")\n",
        "doc = twenty_test.data[0]\n",
        "print(type(test_t))\n",
        "from eli5.lime import TextExplainer\n",
        "# from skater.core.local_interpretation.lime.lime_text import LimeTextExplainer\n",
        "classes = [0,1]\n",
        "te = TextExplainer(random_state = 42)\n",
        "\n",
        "te.fit(x1, new_model.predict)\n",
        "te.show_prediction(target_names = classes)"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.94169587 0.05830408]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.94169587 0.05830408]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.96204644 0.03795353]\n",
            " [0.9380862  0.06191375]\n",
            " [0.94169587 0.05830408]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.96204644 0.03795353]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.96204644 0.03795353]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.96204644 0.03795353]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.94169587 0.05830408]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]\n",
            " [0.94169587 0.05830408]\n",
            " [0.9380862  0.06191375]\n",
            " [0.9380862  0.06191375]]\n",
            "<class 'str'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "    <style>\n",
              "    table.eli5-weights tr:hover {\n",
              "        filter: brightness(85%);\n",
              "    }\n",
              "</style>\n",
              "\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "        \n",
              "\n",
              "    \n",
              "\n",
              "        \n",
              "\n",
              "        \n",
              "    \n",
              "        \n",
              "        \n",
              "    \n",
              "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
              "            <b>\n",
              "    \n",
              "        y=0\n",
              "    \n",
              "</b>\n",
              "\n",
              "    \n",
              "    (probability <b>0.951</b>, score <b>-2.966</b>)\n",
              "\n",
              "top features\n",
              "        </p>\n",
              "    \n",
              "    <table class=\"eli5-weights\"\n",
              "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
              "        <thead>\n",
              "        <tr style=\"border: none;\">\n",
              "            \n",
              "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
              "                    Contribution<sup>?</sup>\n",
              "                </th>\n",
              "            \n",
              "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
              "            \n",
              "        </tr>\n",
              "        </thead>\n",
              "        <tbody>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +2.094\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        Highlighted in text (sum)\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 89.18%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.871\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        &lt;BIAS&gt;\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "        \n",
              "\n",
              "        \n",
              "        \n",
              "\n",
              "        </tbody>\n",
              "    </table>\n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n",
              "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
              "        <span style=\"background-color: hsl(120, 100.00%, 76.13%); opacity: 0.90\" title=\"0.078\">quebec</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 62.50%); opacity: 0.98\" title=\"-0.149\">nationalists</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 68.43%); opacity: 0.94\" title=\"-0.116\">see</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.12%); opacity: 0.80\" title=\"-0.001\">province</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 60.31%); opacity: 1.00\" title=\"-0.161\">nation</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 60.00%); opacity: 1.00\" title=\"0.163\">1960s</span>\n",
              "    </p>\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    }
  ]
}